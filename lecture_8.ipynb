{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a97ba5e228f2efc"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:15.651864Z",
     "start_time": "2024-04-22T14:16:15.580506Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV , HalvingGridSearchCV, HalvingRandomSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import get_scorer_names, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from scipy.stats import beta, loguniform"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:16.161172Z",
     "start_time": "2024-04-22T14:16:16.118604Z"
    }
   },
   "id": "ca2cac4a56d0d16d",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data preparation\n",
    "data = pd.read_csv('data/breast.data', header=None)\n",
    "X = data.loc[:,2:].values\n",
    "y = data.loc[:,1].map({'M':1, 'B':0}).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:16.529404Z",
     "start_time": "2024-04-22T14:16:16.188686Z"
    }
   },
   "id": "4c8a9363f37c1c73",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_test_scaled = std_scaler.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:16.602479Z",
     "start_time": "2024-04-22T14:16:16.538964Z"
    }
   },
   "id": "550fbd79b10d038b",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6027f48e6d3254b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The class implementing Grid search in SKLearn is GridSearchCV, from the sklearn.model_selection module.\n",
    "The first step is to define the values which each hyperparameter can assume and define a grid, usually through a dictionary.\n",
    "\n",
    "Important: the keys of the dictionary correspond to the names of the hyperparameters of the classifier, i.e. the parameters of the constructor. In the case of Pipeline objects the game is harder, since we have to use a special syntax for indicating the parameters of a specific element in the pipeline.\n",
    "Here we evaluate the performance acting only on the classifier hyperparameter, LogisticRegression in this case. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b07273430a5d76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:16.619744Z",
     "start_time": "2024-04-22T14:16:16.607203Z"
    }
   },
   "id": "718d36541fda12ed",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we define the classifier. Since the candidates for the penalty parameter are l1 and l2, we have to change the default solver for Logistic Regression, from 'lbfgs' to 'liblinear'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9bc54d2594704f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = LogisticRegression(solver='liblinear')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:16.783300Z",
     "start_time": "2024-04-22T14:16:16.771264Z"
    }
   },
   "id": "33033daae50ef9c9",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=cls, \n",
    "                  param_grid=param_grid,\n",
    "                  scoring='f1',\n",
    "                  refit=True,\n",
    "                  cv=10,\n",
    "                  verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:16.970980Z",
     "start_time": "2024-04-22T14:16:16.932981Z"
    }
   },
   "id": "4297b4aeac2ebb40",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score we got from the best estimator: 0.9759145021645022\n",
      "Configuration for the best estimator/classifier: {'C': 1.0, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "gs = gs.fit(X_train_scaled, y_train)\n",
    "print(f'Best score we got from the best estimator: {gs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {gs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:35.345201Z",
     "start_time": "2024-04-22T14:16:17.211600Z"
    }
   },
   "id": "d22e1bf3232ba0f3",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "GridSearchCV uses k-fold cross-validation for comparing the models associated with the different hyperparameter configurations. As for cross-validation, we can specify the performance metric for selecting the best classifier. Here, we used F1-score.\n",
    "\n",
    "After cross-validation, we can get the score for the best fitting configuration by attribute best_score_ and the corresponding hyparams by the attribute best_params_.\n",
    "\n",
    "Finally, by the attribute best_estimator_, we get the predictor object which got the best performance in CV. \n",
    "We don't need to re-train the model, because it's already done by GridSearchCV by default (parameter refit)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b96255fd4849b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9647058823529412"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, gs.best_estimator_.predict(X_test_scaled))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:35.366864Z",
     "start_time": "2024-04-22T14:16:35.348146Z"
    }
   },
   "id": "1d9e5fbded503528",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we computed the F1-score on the test set using the best estimator returned by GridSearchCV."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ecd5b42ebb3772"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In SKLEarn, the randomised grid search is implemented by the class RandomizedSearchCV in the module sklearn.model_selection.\n",
    "\n",
    "In this case, we have to specify how we sample the values for each hyperparam. For each hyperparam, we define a probability distribution used for the sampling.\n",
    "\n",
    "Suppose we have 3 hyperparameters p1, p2, p3, distributed according to P1, P2, P3, respectively. At each step, we extract a value from P1, a value from P2 and a value from P3 and make a triple of hyperparams. Extractions are independent.\n",
    "\n",
    "The main difference with GridSearchCV is that we have to specify distributions. in the following example, we use Perceptron to show how to define a probability distribution on a parameter (learning rate, eta) and a uniform distribution over a list of values (epochs). For the learning rate (eta) we choose the Beta distribution.\n",
    "\n",
    "In general, we can use any object which implements the method rvs(). All the distributions in scipy.stats fullfill this requirement."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d538824e4e67b90e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.78543593, 0.16139742, 0.37870956, 0.34458664, 0.34938546,\n       0.37593123, 0.89196078, 0.73004436, 0.60463728, 0.4797088 ])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta(2,2).rvs(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:35.388675Z",
     "start_time": "2024-04-22T14:16:35.369494Z"
    }
   },
   "id": "e17bedc232cdb11e",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = Perceptron()\n",
    "param_grid = {\n",
    "    'eta0': beta(2, 2),\n",
    "    'max_iter': [10, 30, 40, 100, 500, 1000]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:35.399008Z",
     "start_time": "2024-04-22T14:16:35.392715Z"
    }
   },
   "id": "73d7ba36e464799a",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score we got from the best estimator: 0.9637770562770562\n",
      "Configuration for the best estimator/classifier: {'eta0': 0.827280711292405, 'max_iter': 10}\n"
     ]
    }
   ],
   "source": [
    "rs = RandomizedSearchCV(estimator=cls,\n",
    "                        param_distributions=param_grid,\n",
    "                        scoring='f1',\n",
    "                        refit=True,\n",
    "                        n_iter=20,\n",
    "                        cv=10,\n",
    "                        random_state=1,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1)\n",
    "rs = rs.fit(X_train_scaled, y_train)\n",
    "print(f'Best score we got from the best estimator: {rs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {rs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:38.490942Z",
     "start_time": "2024-04-22T14:16:35.401419Z"
    }
   },
   "id": "bf25add06811ca3f",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll now use random grid search with Logistic Regression to verify if this search can identify a better configuration not taken into account by the grid search."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818a699ff10486c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At each iteration (step), the size of training set increases (n_samples) and the number of candidate configurations decreases (n_candidates).\n",
    "\n",
    "In SKLearn SH search is implemented by the classes HalvingSearchCV and HalvingRandomSearchCV. Both classes are still experimental, so we have to enable them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbccef2a890f8a37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = LogisticRegression(solver='liblinear')\n",
    "param_grid = {\n",
    "    'C': loguniform(0.0001, 1000),\n",
    "    'penalty':['l1', 'l2']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:38.503594Z",
     "start_time": "2024-04-22T14:16:38.494057Z"
    }
   },
   "id": "1cea42d3f8f6a78d",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score we got from the best estimator: 0.9643835616438355\n",
      "Configuration for the best estimator/classifier: {'C': 0.013071577689307423, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "hs = HalvingRandomSearchCV(\n",
    "    cls, \n",
    "    param_distributions=param_grid,\n",
    "    n_candidates='exhaust',\n",
    "    resource='n_samples',\n",
    "    factor=1.2,\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "hs = hs.fit(X_train_scaled, y_train)\n",
    "print(f'Best score we got from the best estimator: {hs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {hs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:40.047871Z",
     "start_time": "2024-04-22T14:16:38.506995Z"
    }
   },
   "id": "3f83ac3043f8022a",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "16.666666666666657"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - 100/1.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:40.059342Z",
     "start_time": "2024-04-22T14:16:40.051710Z"
    }
   },
   "id": "858b2bd2e37c9f25",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "By the factor parameter we determine how many candidates are eliminated in each iteration: 100% - 100% / factor.\n",
    "\n",
    "Via the resource parameter we specify which is the resource we increment at each iteration.\n",
    "\n",
    "By n_candidates, we determine the number of candidate configurations in the first round. The value exhaust indicates that the number of candidates in the last round will be evaluated on the entire training set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd7f9c2b14d430d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9647058823529412"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, hs.best_estimator_.predict(X_test_scaled))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:40.077491Z",
     "start_time": "2024-04-22T14:16:40.063515Z"
    }
   },
   "id": "a7da313d7ba8458b",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following we apply nested cross-validation selecting Logistic Regression as classifier, and in the inner loop we use randomized grid search strategy to find the best hyperparameter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a033c0d4c1d2f1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "CV F1-score: 0.960 +/- 0.027\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': loguniform(0.0001, 1000),\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "hs_log = RandomizedSearchCV(estimator=LogisticRegression(solver='liblinear'),\n",
    "                            param_distributions=param_grid,\n",
    "                            scoring='f1',\n",
    "                            n_iter=50,\n",
    "                            cv=2,\n",
    "                            verbose=1\n",
    ")\n",
    "\n",
    "scores = cross_val_score(hs_log, X_train_scaled, y_train,\n",
    "                         scoring='f1', cv=5, verbose=1)\n",
    "print(f'CV F1-score: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:43.151043Z",
     "start_time": "2024-04-22T14:16:40.083056Z"
    }
   },
   "id": "9c38acd2110372a9",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, it's time to test a KNN classifier, optimizing on K. In SKLearn the KNN classifier is implemented by the class KNeighborsClassifier in the sklearn.neighbors module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db48d1b62b1e7333"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "CV F1-score: 0.951 +/- 0.040\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
    "}\n",
    "\n",
    "hs_knn = RandomizedSearchCV(estimator=KNeighborsClassifier(),\n",
    "                            param_distributions=param_grid,\n",
    "                            scoring='f1',\n",
    "                            cv=2,\n",
    "                            verbose=1\n",
    ")\n",
    "\n",
    "scores = cross_val_score(hs_knn, X_train_scaled, y_train,\n",
    "                         scoring='f1', cv=5, verbose=1)\n",
    "print(f'CV F1-score: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T14:16:43.798698Z",
     "start_time": "2024-04-22T14:16:43.153618Z"
    }
   },
   "id": "cc13b614484783f5",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter optimization on a pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d198ff0334dc327c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the above strategies can generalize to Pipeline objects with Predictor in the final step of the pipeline. The main difference w.r.t. the above examples is that we need a way to indicate the Hyperparameters of the different elements in the pipeline.\n",
    "\n",
    "In this case, we have to remember that each element in a pipeline has an identifier - a string - associated to its Transformer/Predictor object. This way we just nee da syntax to indicate a hyperparameter name belonging to a specific identifier.\n",
    "\n",
    "In SKLearn, we use the string _ to indicate this dependency.\n",
    "For instance, given a Pipeline:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a93f7d0d2a8b98e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "p = Pipeline(\n",
    "    ('dim_reducer',PCA()),\n",
    "    ('classifier',LogisticRegression())\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "260c2b5c4a42144b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "we select the parameter n_components of the PCA object through this syntax:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397537a179a127f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "dim_reducer__n_components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39c9a7837df59d5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this syntax, we may run a (randomized) grid search on the entire pipeline, putting into the hyperparameter space all the yperparameters of the elements in the pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185cd023cfce00ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pipeline_log = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('feat_selection', PCA()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'feat_selection__n_components': [0.3, 0.5, 0.7, 0.9, 1],\n",
    "        'classifier__penalty': ['l1'],\n",
    "        'classifier__C': loguniform(0.0001, 1000),\n",
    "        'classifier__solver': ['libllinear']\n",
    "    },\n",
    "    {\n",
    "        'feat_selection__n_components': [0.3, 0.5, 0.7, 0.9, 1],\n",
    "        'classifier__penalty': ['l2'],\n",
    "        'classifier__C': loguniform(0.0001, 1000)\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T20:14:22.506781Z",
     "start_time": "2024-04-22T20:14:22.408519Z"
    }
   },
   "id": "b76f084fa6cd484d",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
      "Best score we got from the best estimator: 0.9316436065985402\n",
      "Configuration for the best estimator/classifier: {'classifier__C': 374.8819462573172, 'classifier__penalty': 'l2', 'feat_selection__n_components': 0.5}\n"
     ]
    }
   ],
   "source": [
    "rs = RandomizedSearchCV(estimator=pipeline_log,\n",
    "                        param_distributions=params,\n",
    "                        scoring='f1',\n",
    "                        refit=True,\n",
    "                        n_iter=50,\n",
    "                        cv=10,\n",
    "                        random_state=1,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1)\n",
    "rs = rs.fit(X_train, y_train)\n",
    "print(f'Best score we got from the best estimator: {rs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {rs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T20:16:04.933497Z",
     "start_time": "2024-04-22T20:16:00.229810Z"
    }
   },
   "id": "4b148897d375af74",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9382716049382716"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, rs.best_estimator_.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T20:16:30.869163Z",
     "start_time": "2024-04-22T20:16:30.805239Z"
    }
   },
   "id": "349a7c67f298bab5",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "307836cb48775a71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
