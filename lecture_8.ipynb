{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a97ba5e228f2efc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:13:38.134198Z",
     "start_time": "2024-04-23T20:13:38.127775Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV , HalvingGridSearchCV, HalvingRandomSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import get_scorer_names, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from scipy.stats import beta, loguniform"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:13:40.573510Z",
     "start_time": "2024-04-23T20:13:38.158827Z"
    }
   },
   "id": "ca2cac4a56d0d16d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data preparation\n",
    "data = pd.read_csv('data/breast.data', header=None)\n",
    "X = data.loc[:,2:].values\n",
    "y = data.loc[:,1].map({'M':1, 'B':0}).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:13:40.626997Z",
     "start_time": "2024-04-23T20:13:40.576604Z"
    }
   },
   "id": "4c8a9363f37c1c73",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_test_scaled = std_scaler.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:13:40.639516Z",
     "start_time": "2024-04-23T20:13:40.629310Z"
    }
   },
   "id": "550fbd79b10d038b",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6027f48e6d3254b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The class implementing Grid search in SKLearn is GridSearchCV (this class implements cross-validation part as well), from the sklearn.model_selection module.\n",
    "The first step is to define the values which each hyperparameter can assume and define a grid, usually through a dictionary.\n",
    "\n",
    "Important: the keys of the dictionary correspond to the names of the hyperparameters of the classifier, i.e. the parameters of the constructor. In the case of Pipeline objects the game is harder, since we have to use a special syntax for indicating the parameters of a specific element in the pipeline.\n",
    "\n",
    "Here we evaluate the performance acting only on the classifier hyperparameter, LogisticRegression in this case. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b07273430a5d76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] # C = strength of regularization term\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:23:53.199103Z",
     "start_time": "2024-04-23T20:23:53.136969Z"
    }
   },
   "id": "718d36541fda12ed",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we define the classifier. Since the candidates for the penalty parameter are l1 and l2, we have to change the default solver for Logistic Regression, from 'lbfgs' to 'liblinear'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9bc54d2594704f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = LogisticRegression(solver='liblinear') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:28:46.743728Z",
     "start_time": "2024-04-23T20:28:46.699816Z"
    }
   },
   "id": "33033daae50ef9c9",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=cls, # MUST specify predictor\n",
    "                  param_grid=param_grid, # specify all the hyperparameters we want to optimize with the values we want to assign\n",
    "                  scoring='f1', # we have to optimize by using scoring function f1\n",
    "                  refit=True, # after the optimization, the class selects the model which maximizes the scoring function and does retraining\n",
    "                  cv=10, # with cv part, we train model [number of combinations of hyperparameters * number of folds]\n",
    "                  verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:28:47.221816Z",
     "start_time": "2024-04-23T20:28:47.209736Z"
    }
   },
   "id": "4297b4aeac2ebb40",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "The object returned by GridSearchCV object is predictor (estimator for sure). Meaning, it has fit() method. The model is the best one selected, but we don't know which one is it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29474bca68b34f10"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score we got from the best estimator: 0.9759145021645022\n",
      "Configuration for the best estimator/classifier: {'C': 1.0, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "gs = gs.fit(X_train_scaled, y_train)\n",
    "print(f'Best score we got from the best estimator: {gs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {gs.best_params_}') # return the object, the predictor which corresponds to the best model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:29:05.074283Z",
     "start_time": "2024-04-23T20:28:48.640869Z"
    }
   },
   "id": "d22e1bf3232ba0f3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "How can we evaluate the overfitting of the model? We get the same performance measure, we compute it on the test, we compare performances on the train and on the test and if the difference is very high, there is an overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dcc22fd7f31530e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9647058823529412"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = std_scaler.transform(X_test) # fit is NOT allowed here!\n",
    "y_predict = gs.predict(X_test_scaled)\n",
    "f1_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:51:08.674501Z",
     "start_time": "2024-04-23T20:51:08.654499Z"
    }
   },
   "id": "caef05e1f76e590b",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "GridSearchCV uses k-fold cross-validation for comparing the models associated with the different hyperparameter configurations. As for cross-validation, we can specify the performance metric for selecting the best classifier. Here, we used F1-score.\n",
    "\n",
    "After cross-validation, we can get the score for the best fitting configuration by attribute best_score_ and the corresponding hyparams by the attribute best_params_.\n",
    "\n",
    "Finally, by the attribute best_estimator_, we get the predictor object which got the best performance in CV. \n",
    "We don't need to re-train the model, because it's already done by GridSearchCV by default (parameter refit)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b96255fd4849b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9647058823529412"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, gs.best_estimator_.predict(X_test_scaled))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:53:28.710301Z",
     "start_time": "2024-04-23T20:53:28.681891Z"
    }
   },
   "id": "1d9e5fbded503528",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we computed the F1-score on the test set using the best estimator returned by GridSearchCV."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ecd5b42ebb3772"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Randomized Grid Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d9d899a0207f022"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In SKLEarn, the randomised grid search is implemented by the class RandomizedSearchCV in the module sklearn.model_selection.\n",
    "\n",
    "In this case, we have to specify how we sample the values for each hyperparam. For each hyperparam, we define a probability distribution used for the sampling.\n",
    "\n",
    "Suppose we have 3 hyperparameters p1, p2, p3, distributed according to P1, P2, P3, respectively. At each step, we extract a value from P1, a value from P2 and a value from P3 and make a triple of hyperparams. Extractions are independent.\n",
    "\n",
    "The main difference with GridSearchCV is that we have to specify distributions. in the following example, we use Perceptron to show how to define a probability distribution on a parameter (learning rate, eta) and a uniform distribution over a list of values (epochs). For the learning rate (eta) we choose the Beta distribution.\n",
    "\n",
    "In general, we can use any object which implements the method rvs(). All the distributions in scipy.stats fullfill this requirement."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d538824e4e67b90e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.29037926, 0.3241965 , 0.79418224, 0.25102711, 0.0777429 ,\n       0.24476125, 0.53684427, 0.32978046, 0.07859346, 0.02988539])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta(2,2).rvs(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:55:54.139994Z",
     "start_time": "2024-04-23T20:55:54.073242Z"
    }
   },
   "id": "e17bedc232cdb11e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = Perceptron()\n",
    "param_grid = {\n",
    "    'eta0': beta(2, 2), # we can generate number according to the distribution!\n",
    "    'max_iter': [10, 30, 40, 100, 500, 1000]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:11:12.311269Z",
     "start_time": "2024-04-23T21:11:12.263642Z"
    }
   },
   "id": "73d7ba36e464799a",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/aleksandrak/Desktop/ML-course-Labs/.venv/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score we got from the best estimator: 0.9637770562770562\n",
      "Configuration for the best estimator/classifier: {'eta0': 0.827280711292405, 'max_iter': 10}\n"
     ]
    }
   ],
   "source": [
    "rs = RandomizedSearchCV(estimator=cls,\n",
    "                        param_distributions=param_grid,\n",
    "                        scoring='f1',\n",
    "                        refit=True,\n",
    "                        n_iter=20,\n",
    "                        cv=10,\n",
    "                        random_state=1,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1)\n",
    "rs = rs.fit(X_train_scaled, y_train)\n",
    "print(f'Best score we got from the best estimator: {rs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {rs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:11:17.159799Z",
     "start_time": "2024-04-23T21:11:12.954859Z"
    }
   },
   "id": "bf25add06811ca3f",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll now use random grid search with Logistic Regression to verify if this search can identify a better configuration not taken into account by the grid search."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818a699ff10486c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Successive Halving Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82192fb9191823ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At each iteration (step), the size of training set increases (n_samples) and the number of candidate configurations decreases (n_candidates).\n",
    "\n",
    "In SKLearn SH search is implemented by the classes HalvingSearchCV and HalvingRandomSearchCV. Both classes are still experimental, so we have to enable them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbccef2a890f8a37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls = LogisticRegression(solver='liblinear')\n",
    "param_grid = {\n",
    "    'C': loguniform(0.0001, 1000),\n",
    "    'penalty':['l1', 'l2']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:11:23.183047Z",
     "start_time": "2024-04-23T21:11:23.172089Z"
    }
   },
   "id": "1cea42d3f8f6a78d",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score we got from the best estimator: 0.9643835616438355\n",
      "Configuration for the best estimator/classifier: {'C': 0.013071577689307423, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "hs = HalvingRandomSearchCV(\n",
    "    cls, \n",
    "    param_distributions=param_grid,\n",
    "    n_candidates='exhaust',\n",
    "    resource='n_samples', # n_samples - parameter resource\n",
    "    factor=1.2, # removal of the worst is guided by this parameter\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ") # no cv here\n",
    "\n",
    "hs = hs.fit(X_train_scaled, y_train)\n",
    "print(f'Best score we got from the best estimator: {hs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {hs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:11:25.440888Z",
     "start_time": "2024-04-23T21:11:23.924559Z"
    }
   },
   "id": "3f83ac3043f8022a",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "16.666666666666657"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - 100/1.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:18:11.561735Z",
     "start_time": "2024-04-23T21:18:11.547231Z"
    }
   },
   "id": "858b2bd2e37c9f25",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "By the factor parameter we determine how many candidates are eliminated in each iteration: 100% - 100% / factor.\n",
    "\n",
    "Via the resource parameter we specify which is the resource we increment at each iteration.\n",
    "\n",
    "By n_candidates, we determine the number of candidate configurations in the first round. The value exhaust indicates that the number of candidates in the last round will be evaluated on the entire training set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd7f9c2b14d430d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9647058823529412"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, hs.best_estimator_.predict(X_test_scaled))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:25:08.550366Z",
     "start_time": "2024-04-23T21:25:08.523274Z"
    }
   },
   "id": "a7da313d7ba8458b",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Selection with Nested Cross-Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d781c89e04ebb819"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following we apply nested cross-validation selecting Logistic Regression as classifier, and in the inner loop we use randomized grid search strategy to find the best hyperparameter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a033c0d4c1d2f1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "CV F1-score: 0.963 +/- 0.025\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': loguniform(0.0001, 1000),\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# this is the inner loop\n",
    "hs_log = RandomizedSearchCV(estimator=LogisticRegression(solver='liblinear'),\n",
    "                            param_distributions=param_grid,\n",
    "                            scoring='f1',\n",
    "                            n_iter=50,\n",
    "                            cv=2,\n",
    "                            verbose=1\n",
    ")\n",
    "\n",
    "scores = cross_val_score(hs_log, X_train_scaled, y_train,\n",
    "                         scoring='f1', cv=5, verbose=1) # this method implements the outer loop; we evaluate cv using f1\n",
    "print(f'CV F1-score: {np.mean(scores):.3f} +/- {np.std(scores):.3f}') # we get 5 best candidates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:25:15.021220Z",
     "start_time": "2024-04-23T21:25:12.171706Z"
    }
   },
   "id": "9c38acd2110372a9",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, it's time to test a KNN classifier, optimizing on K. In SKLearn the KNN classifier is implemented by the class KNeighborsClassifier in the sklearn.neighbors module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db48d1b62b1e7333"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "CV F1-score: 0.951 +/- 0.040\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
    "}\n",
    "\n",
    "hs_knn = GridSearchCV(estimator=KNeighborsClassifier(),\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='f1',\n",
    "                            cv=2,\n",
    "                            verbose=1\n",
    ")\n",
    "\n",
    "scores = cross_val_score(hs_knn, X_train_scaled, y_train,\n",
    "                         scoring='f1', cv=5, verbose=1)\n",
    "# the point is, we can extract 5 best models out of all the possible ones\n",
    "print(f'CV F1-score: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T22:08:05.667356Z",
     "start_time": "2024-04-23T22:08:05.106100Z"
    }
   },
   "id": "cc13b614484783f5",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter optimization on a pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d198ff0334dc327c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the above strategies can generalize to Pipeline objects with Predictor in the final step of the pipeline. The main difference w.r.t. the above examples is that we need a way to indicate the Hyperparameters of the different elements in the pipeline.\n",
    "\n",
    "In this case, we have to remember that each element in a pipeline has an identifier - a string - associated to its Transformer/Predictor object. This way we just nee da syntax to indicate a hyperparameter name belonging to a specific identifier.\n",
    "\n",
    "In SKLearn, we use the string _ to indicate this dependency.\n",
    "For instance, given a Pipeline:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a93f7d0d2a8b98e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "p = Pipeline(\n",
    "    ('dim_reducer',PCA()),\n",
    "    ('classifier',LogisticRegression())\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "260c2b5c4a42144b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "we select the parameter n_components of the PCA object through this syntax:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397537a179a127f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "dim_reducer__n_components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39c9a7837df59d5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this syntax, we may run a (randomized) grid search on the entire pipeline, putting into the hyperparameter space all the yperparameters of the elements in the pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185cd023cfce00ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pipeline_log = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('feat_selection', PCA()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'feat_selection__n_components': [0.3, 0.5, 0.7, 0.9, 1],\n",
    "        'classifier__penalty': ['l1'],\n",
    "        'classifier__C': loguniform(0.0001, 1000),\n",
    "        'classifier__solver': ['libllinear']\n",
    "    },\n",
    "    {\n",
    "        'feat_selection__n_components': [0.3, 0.5, 0.7, 0.9, 1],\n",
    "        'classifier__penalty': ['l2'],\n",
    "        'classifier__C': loguniform(0.0001, 1000)\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:25:17.018709Z",
     "start_time": "2024-04-23T21:25:17.004311Z"
    }
   },
   "id": "b76f084fa6cd484d",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
      "Best score we got from the best estimator: 0.9316436065985402\n",
      "Configuration for the best estimator/classifier: {'classifier__C': 374.8819462573172, 'classifier__penalty': 'l2', 'feat_selection__n_components': 0.5}\n"
     ]
    }
   ],
   "source": [
    "rs = RandomizedSearchCV(estimator=pipeline_log,\n",
    "                        param_distributions=params,\n",
    "                        scoring='f1',\n",
    "                        refit=True,\n",
    "                        n_iter=50,\n",
    "                        cv=10,\n",
    "                        random_state=1,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1)\n",
    "rs = rs.fit(X_train, y_train)\n",
    "print(f'Best score we got from the best estimator: {rs.best_score_}')\n",
    "print(f'Configuration for the best estimator/classifier: {rs.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:25:21.396588Z",
     "start_time": "2024-04-23T21:25:17.745820Z"
    }
   },
   "id": "4b148897d375af74",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9382716049382716"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, rs.best_estimator_.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T21:25:21.411725Z",
     "start_time": "2024-04-23T21:25:21.399343Z"
    }
   },
   "id": "349a7c67f298bab5",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "307836cb48775a71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
